% easychair.tex,v 3.5 2017/03/15

\documentclass{easychair}
%\documentclass[EPiC]{easychair}
%\documentclass[EPiCempty]{easychair}
%\documentclass[debug]{easychair}
%\documentclass[verbose]{easychair}
%\documentclass[notimes]{easychair}
%\documentclass[withtimes]{easychair}
%\documentclass[a4paper]{easychair}
%\documentclass[letterpaper]{easychair}

\usepackage{doc}
\usepackage{fullpage}
\usepackage{latexsym}
\usepackage{verbatim}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{stackengine}
\usepackage{scalerel}
\usepackage{code,proof,amsthm,amssymb, amsmath}
\usepackage{mathpartir}
\usepackage{turnstile}
\usepackage{fancyvrb}
\usepackage{stmaryrd}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{adjustbox}
\usepackage{color}
\usetikzlibrary{positioning} 

\allowdisplaybreaks

\input{../generic-defns}
\input{../syn-defns}
\input{../../pfpl/fun-defns}
\input{../../pfpl/pcf-defns}
\input{../../pfpl/prod-defns}
\input{../../pfpl/sum-defns}
\input{../../pfpl/icoi-defns}
\input{../../pfpl/t-defns}


% use this if you have a long article and want to create an index
% \usepackage{makeidx}

% In order to save space or manage large tables or figures in a
% landcape-like text, you can use the rotating and pdflscape
% packages. Uncomment the desired from the below.
%
% \usepackage{rotating}
% \usepackage{pdflscape}

% Some of our commands for this guide.
%
\newcommand{\easychair}{\textsf{easychair}}
\newcommand{\miktex}{MiK{\TeX}}
\newcommand{\texniccenter}{{\TeX}nicCenter}
\newcommand{\makefile}{\texttt{Makefile}}
\newcommand{\latexeditor}{LEd}

\newcommand{\myname}{Andrew Carnegie}
\newcommand{\myandrewid}{andrew}
\newcommand{\hwnumber}{1}
% =========================================================================== %

\newcounter{group}
\setcounter{group}{1}
\newtheorem{theorem}{Task}[group]
% Remove '\newpage' below to preview your doc compactly.
% Remember to put it back before submitting to Gradescope.
\newcommand{\task}{\newpage\begin{theorem}\end{theorem}}
\newcommand{\nextgroup}{\stepcounter{group}}
\newcommand{\skipaheadtask}{\stepcounter{theorem}}
\newcommand{\ms}[1]{\ensuremath{\mathsf{#1}}}
\newcommand{\irl}[1]{\mathtt{#1}}
\newcounter{rule}
\setcounter{rule}{0}
\newcommand{\rn}
  {\addtocounter{rule}{1}(\arabic{rule})}	

\newcounter{infercount}
\setcounter{infercount}{1}
\newcommand{\infern}[2]{\inferrule{#1}{#2}(\text{S}_{\arabic{infercount}}\stepcounter{infercount})}
\newcommand*\ts[2]{%
  \,\scalebox{1}[0.5]{$\sststile[ss]{\textstyle#1}{\textstyle#2}$}\,
}
\newcommand{\inferr}[2]{\inferrule{#2}{#1}}
\newcommand{\inferrr}[3]{\inferrule[#1]{#2}{#3}}
\newcommand{\paircaseabt}[4]{\irl{case}(#2,#3.#4)}
\newcommand{\paircasecst}[4]{\irl{case} \; #1\; \{(#2;#3) \hookrightarrow #4\}}
\newcommand{\na}[1]{\mathsf{linear}(#1)}
\newcommand{\nr}[1]{\mathsf{no\_ref}(#1)}
\newcommand{\stable}[1]{\mathsf{stable}(#1)}
\newcommand{\set}[1]{\mathsf{set}(#1)}
\newcommand{\safe}[1]{\mathsf{safe}(#1)}
\newcommand{\dist}[1]{\mathsf{disjoint}(#1)}
\newcommand{\stack}[1]{\irl{stack}(#1)}
\newcommand{\denote}[1]{\llbracket#1\rrbracket}
\newcommand{\nil}{[]}
\newcommand{\cons}[2]{\pi(#1,#2)}
\newcommand{\sharecst}[4]{\irl{share}\;#1\;\irl{as}\;#2,#3\;\irl{in}\;#4}
\newcommand{\sharecpcst}[4]{\irl{share}\;#1\;\irl{as}\;#2,#3\;\irl{in}\;#4}
\newcommand{\shareabt}[4]{\irl{share}(#1;#2,#3.#4)}
\newcommand{\ssize}[2]{\left\Vert #2 \right\Vert_{#1}}
\newcommand{\card}[1]{card(#1)}
\newcommand{\val}[1]{\irl{val}(#1)}
\newcommand{\gc}[3]{\mathsf{gc}(#1,#2,#3)}
\newcommand{\wfc}[5]{\mathsf{linear}(#1,#2,#3,#4,#5)}
\newcommand{\veq}[4]{#3 \sim^{#1}_{#2} #4}
\newcommand{\ctxeq}[2]{(#1) \sim (#2)}
\newcommand{\oh}[1]{\oslash(#1)}
\newcommand{\fogc}{\ms{FO}^{gc}}
\newcommand{\jan}[1]{{\color{red} #1}}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{attempt}[theorem]{Attempt}
\newtheorem{corollary}{Corollary}[theorem]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
%\makeindex

%% Front Matter
%%
% Regular title as in the article class.
%
\title{Automatic Space Bound Analysis for Functional Programs with Garbage Collection}

% Authors are joined by \and. Their affiliations are given by \inst, which indexes
% into the list defined using \institute
%
\author{
Yue Niu
\and
Jan Hoffmann
}

% Institutes for affiliations are also joined by \and,
\institute{
  Carnegie Mellon University,
  Pittsburgh, PA, United States\\
  \email{\{yuen,jhoffmann\}@cs.cmu.edu}
 }

%  \authorrunning{} has to be set for the shorter version of the authors' names;
% otherwise a warning will be rendered in the running heads. When processed by
% EasyChair, this command is mandatory: a document without \authorrunning
% will be rejected by EasyChair

\authorrunning{Niu and Hoffmann}

% \titlerunning{} has to be set to either the main title or its shorter
% version for the running heads. When processed by
% EasyChair, this command is mandatory: a document without \titlerunning
% will be rejected by EasyChair
\titlerunning{The {\easychair} Class File}

\begin{document}

\maketitle

\begin{abstract}
  This article introduces a novel system for deriving upper bounds on
  the heap-space requirements of functional programs with garbage
  collection.
  %
  The space cost model is based on a perfect garbage collector that
  immediately deallocates memory cells when they become unreachable.
  %
  Heap-space bounds are derived using type-based automatic amortized
  resource analysis (AARA), a template-based technique that
  efficiently reduces bound inference to linear programming.
  %
	The first technical contribution of the work is a new operational cost
  semantics that models a perfect garbage collector.
  %
  The second technical contribution is an extension of AARA
  to take into account automatic deallocation. A key observation is
  that deallocation of a perfect collector can be modeled with
  destructive pattern matching if data structures are used in a linear
  way. However, the analysis uses destructive pattern matching to
  accurately model deallocation even if data is shared.
  The soundness of the extended AARA with respect to the new cost semantics 
	is proven in two parts via an intermediate linear cost semantics.
  %
  The analysis and the cost semantics have been implemented as an
  extension to Resource Aware ML (RaML). An experimental evaluation
  shows that the system is able to derive tight symbolic heap-space
  bounds for common algorithms. Often the bounds are asymptotic
  improvements over bounds that RaML derives without taking into
  account garbage collection.
\end{abstract}

% The table of contents below is added for your convenience. Please do not use
% the table of contents if you are preparing your paper for publication in the
% EPiC Series or Kalpa Publications series

\setcounter{tocdepth}{3}
{\small
\tableofcontents}

%\section{To mention}
%
%Processing in EasyChair - number of pages.
%
%Examples of how EasyChair processes papers. Caveats (replacement of EC
%class, errors).

%------------------------------------------------------------------------------
\section{Introduction}
\label{sect:introduction}
The memory footprint of a program is an important performance metric
that determines if a program can be safely executed on a given
system. Due to the challenges of modelling garbage collection,
most existing techniques for
automating and guiding the derivation of bounds on the heap memory
requirements assume manual memory management or simply ignore
deallocation in the analysis. As a result, the derived bounds
are not accurate when the underlying system employs garbage
collection.

In the first section, we describe the target language --
first-order, garbage-collected functional programs.
We define the language by an operational cost semantics $\ms{free}$ 
which keeps track of garbage collection. The semantics is a big-step
(or natural) semantics in the style of Spoonhower and Minamide (\cite{Spoonhower:2008:SPP:1411204.1411240} and 
\cite{DBLP:journals/entcs/Minamide99}), which assigns any program a space usage. 
Operationally, this cost is
the high watermark on the heap, or the maximum number of cells used in the mutable store during 
evaluation. While \cite{Spoonhower:2008:SPP:1411204.1411240} and 
\cite{DBLP:journals/entcs/Minamide99} compute this cost at the leaves of a 
evaluation judgment, we compute the cost ``as it happens'' by leveraging the \emph{free-list},
which represents the cells available for evaluation. 
The concept of the free-list can be found in Hofmann et. al. (\cite{Hofmann:2003:SPH:604131.604148}).
However, unlike \cite{Hofmann:2003:SPH:604131.604148}, in order to account for garbage collection,
we define the free-list as a set of locations instead of single number denoting its size.

In the next couple sections, we present a new type system $\fogc$
for statically determining the symbolic cost bound 
of the space complexity for the aforementioned garbage collected programs.
The bound inference is an extension to Automatic Amortized Resource Analysis (AARA) 
and implemented in Resource Aware ML (RaML) ~\cite{Hoffmann:2017:TAR:3009837.3009842}. 
As the the underlying type system is linear, we introduce an auxiliary cost semantics 
$\ms{copy}$ which is exactly the same as $\ms{free}$, except that variables are used in 
a linear way. We prove the soundness of $\ms{copy}$ with respect to $\fogc$, then prove that
$\ms{copy}$ is an over-approximation of $\ms{free}$ in the sense that any program that can be 
successfully completed when interpreted with $\ms{copy}$ can also be completed with $\ms{free}$ 
using the same starting free-list.

Lastly, we discuss the implementation and evaluate the system on a variety of functions. Our 
results suggest that our new analysis provides asymptotic bound improvements to several classes 
of commonly used functions and programming patterns. We then examine the reasons for these 
improvements and design decisions throughout the system. Lastly, we conclude with thoughts for 
 and directions for future work. 

\jan{More stuff in intro}

\iffalse
\section{Notation}
\label{sect:notation}
For a finite mapping $f : A \to B$, we write $dom$ for the defined values of $f$. Sometimes we shorten $x \in dom(f)$ to $x \in f$. We write $f[x \mapsto y]$ for the extension of $f$ where $x$ is mapped to $y$, with the constraint that $x \notin dom(f)$. 

Given possibly non-disjoint sets $A,B$, let the disjoint union be $A \oplus B$ defined by 
$\{(\ms{inl},a) \mid a \in A\} \cup \{(\ms{inr},b) \mid b \in B\}$.

Let a multiset be a function $S : A \to \mathbb{N}$, i.e. a map of the multiplicity of each element in the domain.  Write $x \in S$ iff $S(x) \ge 1$. If for all  $s \in S$, $\mu(s) = 1$, then $S$ 
is a property set, and we denote this by $\ms{set}(S)$. Addtionally, $A \uplus B$ denotes 
counting union of sets where $(A \uplus B) (s) = A (s) + B(s)$, similarly, 
$(A \cap B)(s) = \min{A(s),B(s)}$. Furthermore, $A \cup B$ denotes the usual union where 
$(A \cup B)(s) = \max{(A(s),B(s))}$.  For the union of disjoint multi-sets $A$ and $B$, 
we write $A \sqcup B$ to emphasize the disjointness.  For a collection of pairwise disjoint 
multi-sets $\mathcal{C}$, i.e. $\forall X,Y \in \mathcal{C}$. $X \cap Y = \emptyset$, we write $\dist{\mathcal{C}}$.

In the rest of the paper, 
we sometimes treat a set $A$ sets as multiset $A : A \to \mathbb{N}$ via 
$x \mapsto \begin{cases} 1 &\text{ if } x \in A \\0 &\text{ o.w.}\end{cases}$ when convenient. 
For instance, if an operation defined on multisets is used on sets and multisets, the set 
is thus promoted.

Given a set $A$, let $\mathcal{P}(A)$ be the powerset of $A$. Given a multiset $A$, let 
$\wp(A)$ be the power multiset of $A$, i.e. the set of all submultisets of $A$.

For a partition $f : A \to \mathcal{P}(B)$, we write the set of equivalence classes
as $ec(f) = \{f(x) \mid x \in A\} = f(A)$, i.e. the image of $f$ on its domain $A$.
Furthermore, a partition is \emph{proper} if for any $x \in A$, $f(x) \neq \emptyset$.

Given a proper partition $f : A \to \mathcal{P}(B)$, for every $a \in A$, 
we can choose an arbitrary 
$b \in f(a)$ to be the representative for that part; call this $rep(a)$.
\fi

\section{Setting the Stage}
\label{sect:fop}

In this paper, in order to make clear the ideas and proofs involving our new garbage-collection 
cost semantics, we focus our attention to a first-order, 
strictly evaluated functional language. 
The reader can think of our target language to be a very simple subset of OCaml or SML. 
Furthermore, although we only demonstrate data types in the language with lists, 
our techniques extend to the expected algebraic data types definable in ML; in fact, 
our OCaml implementation leverages the OCaml compiler and supports type declarations in OCaml. 
Being first order, our language does not allow arbitrary local functional definitions. Instead,
all functions are defined in a ``global header'' consisting of $n$ mutually recursive blocks. 
The types of these functions form a signature for the program, and the semantics and typing 
judgments will be indexed by this signature. Thus, the types of the language can be expressed 
as an arrow between zero-order (base)  types: 

\[
\begin{array}{r l l l l}
\ms{BTypes} & \tau \,\,\,\,\, ::= \\
	& \irl{nat}                	 			& \irl{nat}											& \text{naturals}\\
	& \unittyabt                	 			& \unittycst										& \text{unit}\\
  & \booltyabt                       & \booltycst                    & \text{boolean}\\
  & \prodtyabt{\tau_1}{\tau_2}       & \prodtycst{\tau_1}{\tau_2}    & \text{product}\\
  &\listtyabt{\tau}		& L(\tau)											& \text{list}\\
  \\
\ms{FTypes} & \rho \,\,\,\,\, ::= \\
	&\irl{arr}(\tau_1;\tau_2) 				& \arrtycst{\tau_1}{\tau_2} 									& \text{first order function}
\end{array}
\]

Next, we have the (abridged) expressions: 

\[
\begin{array}{r l l l l}
\ms{Exp}
        & e   \,\,\,\,\, ::= \\
 	& x                                			& x 												& \text{variable}\\
  & \irl{nat}[n]							& \numeral{n}												& \text{number}\\
  & \irl{unit}							& ()												& \text{unit}\\
  & \irl{T}							& \irl{T}												& \text{true}\\
  & \irl{F}	   					& \irl{F}												& \text{false}\\
  & \ifexabt{x}{e_1}{e_2} & \ifexcst{x}{e_1}{e_2}  & \text{if}\\
  & \irl{lam}(x:\tau.e) 						&\lambda \; x : \tau. e 		& \text{abstraction}\\
  & \irl{ap}(f;x) 					& \appcst{f}{x} 										& \text{application}\\
  & \irl{tpl}(x_1;x_2)     	& \pairexcst{x_1}{x_2}                									& \text{pair}\\
 	& \paircaseabt{p}{x_1}{x_2}{e_1}					& \paircasecst{p}{x_1}{x_2}{e_1}   	& \text{match pair}\\
 	& \nilexabt					& []   										& \text{nil}\\
 	& \consexabt{x_1}{x_2}					& x_1::x_2   										& \text{cons}\\
 	& \listcaseexabt{l}{e_1}{x}{xs}{e_2}					& \listcaseexcst{l}{e_1}{x}{xs}{e_2}   	& \text{match list}\\
  & \irl{let}(e_1; x : \tau.e_2)			& \irl{let}\; x = e_1 \; \irl{in}\; e_2   	& \text{let}\\
  & \shareabt{x}{x_1}{x_2}{e} &\sharecst{x}{x_1}{x_2}{e} &\text{share}
\end{array}
\]

In this paper, we restrict the language to terms that are in let-normal form to simplify the presentation. We allow an extended syntax in the implementation. 
The one nonstandard construct is the $\sharecst{x}{x_1}{x_2}{e}$, which we will explain in more 
detail in the following sections. We introduce two distinct notions of ``linearity'', one on 
the syntactic level, and one on the semantic level. Syntactic linearity is linearity in 
expression variables, while semantic linearity is linearity in locations, which we introduce next.
Although throughout the sections, semantic 
linearity will depend on the semantics at hand, everything we work with respects syntactic linearity.
The sharing construct allows us to maintain syntactic linearity while working with 
multiple semantics that might or might not respect semantic linearity.\\

In line the previous works on space cost semantics, we employ a heap (also known as 
environment) which persistently binds locations to values (normalized terms).
As usual, we derive the cost of a program from the number of heap locations 
number of heap locations used during execution. Locations is an infinite set of names for addressing
the heap; we used natural numbers in the implementation:

	\[
\begin{array}{r l l l l}
\ms{Val}
        & v   \,\,\,\,\, ::= \\
 	& \irl{val}(n)                                			& n 												& \text{numeric value}\\
 	& \irl{val}(\irl{T})                               			& \irl{T} 								  & \text{true value}\\
 	& \irl{val}(\irl{F})                                			& \irl{F}								  & \text{false value}\\
 	& \irl{val}(\irl{Null})                                  & \irl{Null} 								  & \text{null value}\\
 	& \irl{val}(l)                                			& l 								  & \text{loc value}\\
 	& \irl{val}(\pairexabt{v_1}{v_2})                             & \pairexcst{v_1}{v_2} 								  & \text{pair value}\\
  \\
\ms{Loc} & l   \,\,\,\,\, ::= \\
 	& \irl{loc}(l)                                			& l 												& \text{location}\\\\
\ms{Var} & l   \,\,\,\,\, ::= \\
 	& \irl{var}(x)                                			& x 												& \text{variable}
\end{array}
\]

For the rest of the paper, we assume the following shorthands:

\begin{align*}
	&\ms{Stack} \triangleq \{ V \mid V : \ms{Var} \to \ms{Val} \}\\
	&\ms{Heap} \triangleq \{ H \mid H: \ms{Loc} \to \ms{Val} \}\\
\end{align*}

Also, we refer to a tuple $\mathcal{S} = (V,H) \in \ms{Stack} \times \ms{Heap}$ as a \emph{context},
a tuple $\mathcal{C} = (V,H,R,F) \in \ms{Stack} \times \ms{Heap} \times \mathcal{P}(\ms{Loc})
\times \mathcal{P}(\ms{Loc})$ as a \emph{configuration}, and finally a pair of configuration and 
expression $(\mathcal{C}, e)$ a \emph{computation}.


\subsubsection{Reachability}
\label{sect:reachability}

Before we define the rules for the cost semantics, we relate the heap locations to 
expressions and value with the 3-place reachability relation $reach(H,v,L)$ on $\ms{Heap} \times \ms{Val} \times \wp(\ms{Loc})$. This is read as ``under heap $H$, the value $v$ reaches the multiset 
of locations $L$''. Write $L = reach_H(v)$ to indicate this is a functional relation 
justified by the (valid) mode $(+,+,-)$.

\begin{mathpar}
\inferrule{
	A = reach_H(v_1)\\
	B = reach_H(v_2)
}{
	A \uplus B = reach_H(\pairexcst{v_1}{v_2}) 
} 

\inferrule{
	A = reach_H(H(l))\\
}{
	\{l\} \uplus A = reach_H(l)
} 

\inferrule{
	v \in \mathbb{N} \cup \{\irl{T},\irl{F},\irl{Null}\}
}{
	\emptyset = reach_H(v)
} 
\end{mathpar}

Notice that primitives and types with statically-known sizes are stack-allocated 
($\booltycst, \irl{nat}, \prodtycst{\tau_1}{\tau_2}$) and use no heap cells. 
The notion of reachability naturally lifts to expressions:
\begin{align*}
  &locs_{V,H}(e) = \biguplus\limits_{x \in FV(e)} reach_H(V(x))
\end{align*}
Where $FV : \ms{Exp} \to \mathcal{P}(\ms{Var})$ denotes the set of free-variables of expressions as usual.\\

\subsubsection{Towards the Garbage Collection Cost Semantics}

Now we are ready to give a first attempt to modeling the cost semantics for a
tracing garbage collector. Before we present our new semantics, we explain an
earlier version we tried (this particular one is adapted from  
\cite{DBLP:journals/entcs/Minamide99}):

\[V,H,R \vdash e \Downarrow^s v,H\]

Which can be read as under stack $V \in \ms{Stack}$, heap $H \in ms{Heap}$, 
and continuation set $R \subseteq \ms{Loc}$, $e$ evaluates to $v$ 
and $H'$ using $s$ heap locations. The idea is that $R$ keeps track of the set of locations 
necessary to complete the evaluation \emph{after} $e$ is evaluated (hence the name continuation).
For example, we have the let rule: 

\[
	\inferrule{
		V,H,R \cup locs_{V,H}(x.e_2) \vdash e_1 \Downarrow^{s_1} v_1,H_1\\
		V[x \mapsto v_1],H,R \vdash e_2 \Downarrow^{s_2} v_2,H_2\\
	}{
		V,H,R \vdash \irl{let}(e_1; x : \tau.e_2) \Downarrow^{\max{s_1,s_2}} v_2,H_2
	}
\]

Notice that to evaluate $e_1$, we have to extend the continuation $R$ with locations in $e_2$, which
will be used \emph{after} $e_1$ is evaluated. The total space used is the max of the 
component, indicating that locations used for $e_1$ can be reused for $e_2$. 
This is clear when we look at the variable rule: 

\[
	\inferrule{
		V(x) = v
		}{
			V,H,R \vdash x \Downarrow^{|R \cup reach_H(v)|} v,H
			}
\]

Which states that evaluating a variable $x$ requires the locations reachable from $x$ as well as 
the continuation set $R$. While this way of counting heap locations does model a tracing garbage 
collector, it is not compatible with the existing type systems for amortized analysis. In these
systems, the type rules count the heap locations as data is created, i.e. at each data constructor.
Thus looking up a variable incurs no cost, since it was accounted for during creation. This mismatch
between the dynamics and statics of language prevent us from proving the soundness of the analysis. 
Now, we present our novel cost semantics which solves this issue by combining ``freelist'' 
semantics from \cite{Hofmann:2003:SPH:604131.604148} with the above approach.

%------------------------------------------------------------------------------

\section{Garbage Collection Cost Semantics \ms{free}}
\label{sect:semantics}

The garbage collection cost semantics is defined by a collection of judgement of the form:
\[
\fbox{$\mathcal{C} \; \vdash_{P : \Sigma} e \Downarrow v, H', F'$}
\]

Where $\mathcal{C}$ is a configuration usually written as $V,H,R,F$. 
P is a program with signature $\Sigma : \ms{Var} \to \ms{FTypes}$. 
This can be read as: under stack $V$, heap $H$, continuation set $R$,
free-list $F$, and program $P$ with signature $\Sigma$, the expression $e$ evaluates to $v$, 
and engenders a new heap $H'$ and freelist $F'$. Compared to the previous section, the key
ingredient we added is the freelist, which will serve as the set of available 
locations (for allocation of data constructors). 

A \emph{program} is a $\Sigma$ indexed map $P$ from $\ms{Var}$ to pairs $(y_f,e_f)_{f \in \Sigma}$, where $\Sigma(y_f) = A \to B$, and $\Sigma;y_f : A \vdash e_f : B$ (typing rules are discussed in $\ref{sect:typing}$). We write $P : \Sigma$ to mean $P$ is a program with signature $\Sigma$. Because the signature $\Sigma$ for the mapping of function names to first order functions does not change during evaluation, we drop the subscript $\Sigma$ from $\vdash_{\Sigma}$ when the context of evaluation is clear. Thus the evaluation judgement $\vdash$ is indexed by a signatures $\Sigma$, which is a mutually
recursive block of global first-order declarations to be used during evaluation.
The reason for the name $\ms{free}$ (in contrast to an auxiliary semantics $\ms{copy}$) 
will be explained in the soundness proof where we relate $\ms{free}$ and $\ms{copy}$.

The garbage collection semantics is to designed to model the heap usage of a program running with a 
tracing counting garbage collector: whenever a heap cell becomes unreachable from the 
root set, it becomes collected and added to the free-list as available for reallocation.
As before, the continuation set $R$ represents the set of locations 
required to compute the continuation \emph{excluding} the current expression.
Define the  \emph{root set} as the union of the locations in the continuation set $R$ 
and the locations in the current expression $e$. First, we have E:Var:

\[
	\infern
{ V(x) = v\\
}
{V,H,R,F \; \vdash x \Downarrow v,H,F}
\]

Note that in contrast to the semantics in the previous section, looking up a variable incurs no
cost. This ensures that we will be able prove the soundness of the type system. Next, we have 
E:Let:

\[
	\infern{
	V_1 = V\restriction_{FV(e_1)}\\
  R' = R \cup locs_{V_2,H}(\irl{lam}(x : \tau.e_2))\\
  V_1,H,R',F \vdash e_1 \Downarrow v_1,H_1,F_1\\
	V_2' = (V[x \mapsto v_1])\restriction_{FV(e_2)}\\
  g = \{ l \in H_1 \mid l \notin F_1 \cup R \cup locs_{V_2',H_1}(e_2) \}\\
  V_2',H_1,R, F_1 \cup g \vdash e_2 \Downarrow v_2,H_2,F_2 \\
}{
  V,H,R,F \; \vdash \irl{let}(e_1; x : \tau.e_2) \Downarrow v_2,H_2,F_2
}
\]

This states that, to evaluate a sequence of expressions ($e_1,e_2$), we evaluate the first 
expression with the corresponding restricted stack $V_1$ and a expanded continuation set $R'$. 
The extra locations come from the free variables of $e_2$ (not including the bound variable $x$),
which we cannot collect during the evaluation of $e_1$. Next, we extend the restricted stack 
$V_2$ with the result $v_1$, and evaluate $e_2$ with this stack and the original continuation 
set $R$. Below are the abridged rules:

\begin{mathpar}
\infern{
  V(x) = \irl{T}\\
  g = \{l \in H \mid l \notin F \cup R \cup locs_{V,H}(e_1)\}\\
	V' = V\restriction_{FV(e_1)}\\
  V',H,R,F \cup g\; \vdash e_1 \Downarrow v, H',F'
}{
  V,H,R,F \; \vdash \ifexabt{x}{e_1}{e_2} \Downarrow v, H',F'
}

\infern{
	V(x) = \irl{F}\\
  g = \{l \in H \mid l \notin F \cup R \cup locs_{V,H}(e_2)\}\\
	V' = V\restriction_{FV(e_2)}\\
  V',H,R,F \cup g \; \vdash e_2 \Downarrow v, H',F'
}{
  V,H,R,F \; \vdash \ifexabt{x}{e_1}{e_2} \Downarrow v, H' ,F'
}

% function

\infern{
  V(x) = v'\\
  P(f) = (y_f,e_f)\\
  g = \{l \in H \mid l \notin F \cup R \cup locs_{V,H}(e_f)\}\\
  [y_f \mapsto v'],H,R,F \cup g \; \vdash e_f \Downarrow v,H',F'\\
}{
  V,H,R,F \; \vdash \appcst{f}{x} \Downarrow v,H',F'
}

% lists

\infern{
}{
  V,H,R,F \; \vdash \nilexabt \Downarrow \irl{val(Null)},H,F
} 

\infern{
  v = \pairexcst{V(x_1)}{V(x_2)}\\
	l \in F\\
  H' = H\{l \mapsto v\}
}{
	V,H,R,F \; \vdash \consexcst{x_1}{x_2} \Downarrow l,H' ,F \setminus \{l\}
}



\infern{
  V(x) = \irl{Null}\\
	V' = V\restriction_{FV(e_1)}\\
  g = \{l \in H \mid l \notin F \cup R \cup locs_{V,H}(e_1)\}\\
  V',H,R,F \cup g \; \vdash e_1 \Downarrow v, H',F' \\
}{
  V,H,R,F \; \vdash \listcaseexcst{x}{e_1}{x_h}{x_t}{e_2} \Downarrow v,H',F'
}

\infern{
  V(x) =  l\\
  H(l) = \pairexcst{v_h}{v_t} \\
	V'' = (V[x_h \mapsto v_h, x_t \mapsto v_t])\restriction_{FV(e_2)}\\
  g = \{l \in H \mid l \notin F \cup R \cup locs_{V'',H}(e_2)\}\\
  V'',H,R,F \cup g \; \vdash e_2 \Downarrow v, H',F' \\
}{
  V,H,R,F \; \vdash \listcaseexcst{x}{e_1}{x_h}{x_t}{e_2} \Downarrow v,H',F'
}

\inferr{
  V,H,R,F \; \vdash \sharecst{x}{x_1}{x_2}{e} \Downarrow v,H'',F'
}{
  V = V'[x \mapsto v']\\
  V'[x_1 \mapsto v',x_2 \mapsto v'],H',R,F \; \vdash e \Downarrow v,H'',F'
}(\text{F:Share})
\end{mathpar}

In particular, note that since we don't allow local function definitions, no closures are created 
during evaluation. However, our implementation uses closures to implement the global block of 
function definitions. Also note that we restrict the domain of the stack to the appropriate
variables during evaluation. This is only to facilitate the proof of the linearity of $\ms{copy}$, 
and not necessary for the implementation. These issues are discussed in \ref{sect:implementation}.

For example, we can implement and analyze the following simple dfs algorithm on trees:

\begin{verbatim}
type btree = Leaf | Node of int*btree*btree

let dfs t x =

  let rec dfs_aux queue =
    match queue with
      | [] -> None
      | t::ts -> 
	match t with
          | Leaf -> dfs_aux ts
          | Node(a,t1,t2) -> 
	    if a = x then 
	      Some t 
	    else 
	      dfs_aux (t1::t2::ts)
  in

  dfs_aux [t]
\end{verbatim}

The search starts at the root, and checks if the node is the target. If not, 
the left and right subtrees are queued in that order, and recursively searches.
We now analyze by hand its heap usage under $\ms{free}$. Tree \texttt{t} is given 
as input; to start off the tail-recursive helper, we need to allocate 2 heap locations,
1 for the cons cell with \texttt{t}, and 1 for nil. And that's actually the entire overhead; 
we can show the helper \texttt{dfs\_aux} has no overhead. Note that since the whole program is 
linear (no variable sharing), we know that constructors can be deallocated in pattern matches. 
If \texttt{queue} is nil, then its location is collected and allocated to \texttt{None}.
If \texttt{queue} is \texttt{t::ts}, then we have 2 cases. In case \texttt{t} is a leaf, 
the search continues, with no space used. If \texttt{t} is a node, then the location for the node
is collected and either used to for \texttt{Some t} or used together with the location for 
the cons in \texttt{t::ts} to allocate \texttt{t1::t2::ts}, and the search continues with
no overhead. Since no runs through \texttt{dfs\_aux} require additional heap locations, 
it is zero overhead. Thus, this dfs implementation has a constant space cost of 2.

We can now demonstrate running a closed 
expression. We write a helper function for generating near-balanced trees from lists: 

\begin{verbatim}
(* 
make_btree [1;2;3;4] =
Node ( 1, 
	Node ( 2, 
		Node ( 4,Leaf (), Leaf () ), 
		Leaf () ),
	Node ( 3, Leaf (), Leaf () ))
*)
let make_btree l =

  let rec split l =
    match l with
      | [] -> ([],[])
      | x1::xs ->
	match xs with
	  | [] -> ([x1],[])
	  | x2::xs ->
	    let (a,b) = split xs in
	    (x1::a,x2::b)
  in

  let rec mk_tree l =
    match l with
      | [] -> Leaf
      | x::xs ->
	let (a,b) = split xs in
	let (t1,t2) = (mk_tree a, mk_tree b) in
	Node(x,t1,t2)
  in

  mk_tree l
\end{verbatim}

The follow judgment holds: $\emptyset,\emptyset,\emptyset,\{l_1,...,l_9\} 
	\vdash \texttt{mk\_tree [1;2;3;4]}, l_9, H', \emptyset$, where $H'$ is

\begin{verbatim}
[
	l1 |--> Null
	l2 |--> Null 
	l3 |--> Null 
	l4 |--> Null 
	l5 |--> Null 
	l6 |--> <4, Leaf@[l1], Leaf@[l2]>, 
	l7 |--> <2, Node@[l6], Leaf@[l4]>, 
	l8 |--> <3, Leaf@[l3], Leaf@[l5]>, 
	l9 |--> <1, Node@[l7], Node@[l8]>
]
\end{verbatim}

Next, we can run dfs on this example tree:\\
$\emptyset,\emptyset,\emptyset,\{l_1,...,l_1\} 
	\vdash \texttt{dfs (mk\_tree [1;2;3;4]) 3}, l_1, H', 
	\{l_2, l_4, l_6, l_7, l_9, l_10, l_11,\}$, where $H'$ is

\begin{verbatim}
[
	l1 |--> Node@[l8] 
	l2 |--> <freed>
	l3 |--> Null 
	l4 |--> <freed>
	l5 |--> Null
	l6 |--> <freed>
	l7 |--> <freed>
	l8 |--> <(Cint 3), Leaf@[l3], Leaf@[l5]>, 
	l9 |--> <freed>
	l10 |--> <freed>
	l11 |--> <freed>
]
\end{verbatim}

From this run, we see that the space overhead of dfs is 2 heap locations, which agrees with 
our manual derivation. In the next section, we see that the type system will also be 
able to derive this optimal bound. 

\section{Automatic Amortized Resource Analysis}
\label{sect:aara}

\jan{Better intro for AARA, polynomial potential somewhere}

To introduce AARA, we informally explain the linear potential method for the dfs algorithm.
We will use the allocation/heap metric which simply counts the number of allocations made 
(in contrast to the garbage collection metric, the new analysis which records the highwater mark). 
Given the target function, we associate with each constructor node in the input a
constant potential $p$. 
In the type system, this is computed via constraints generated from type checking, but we can 
just guess and check our assignment. 
Thus, we need to give the potential assignment to \texttt{t} and \texttt{queue}; let these
constants be $p$ and $q$, respectively.
In the body of \texttt{dfs\_aux}, we have
3 places where heap allocations happen: when the queue is nil, the target was found, 
or when target was not found and search happens recursively with the two subtrees in the queue.
The cost of \texttt{None} and \texttt{Some} can be covered if $q = 1$, since constructor nodes
cost unit potential. However, \texttt{t1::t2::ts} costs $2q+2$, since there are 2 cons cells and 
each cell has $q$ units of potential. From the 2 pattern matches, we get $p+q$ units of potential,
so we just have to satisfy the constraint $p+q \ge 2q+2$. One such assignment is $p = 2, q = 0$; 
and indeed, this is the assignment that RaML derives for dfs. Lastly, to finish the analysis,
we need to resolve any constant potential required. The case when \texttt{queue} is nil is 
not covered since nil does not have potential; the initial call to \texttt{dfs\_aux} also 
requires 2 cons constructors (but 0 potential per cons cell). So we need a total of 3 units 
of constant potential. Thus the overall allocation cost of dfs is $2m+3$, where $m$ is the number 
of \texttt{Node} constructors in the input.

\subsubsection{Type Rules}
\label{sect:typing}
The type system $\fogc$ consists of rules of the form \fbox{$\Sigma;\Gamma \sststile{q'}{q} e : A$}, 
read as under signature $\Sigma$, context $\Gamma$, $e$ has type $A$ starting with $q$ units of 
constant potential and ending with $q'$ units.

The type system is based on the affine type system in \ref{Hoffmann:2017:TAR:3009837.3009842}. 
First, we explain the rule L:MatL:

\[
\inferr{
  \Sigma; \Gamma, x : L^p(A) \sststile{q'}{q} \listcaseexcst{x}{e_1}{x_h}{x_t}{e_2} : B
}{
  \Sigma; \Gamma \sststile{q'}{q} e_1 : B \\
  \Sigma; \Gamma, x_h : A, x_t : L^p(A) \sststile{q'}{q + p + 1} e_2 : B
}(\text{L:MatL})
\]

The key insight for the new type system that we can model garbaged collected locations by 
returning potential locally in a match construct. 
This is validated by the fact (Lemma \ref{itm:na}) that once a cons-cell is matched on, 
there can be no live references from the root set to it, and thus 
we are justified in restituting the potential to type the subexpression $e_2$. The next 
interesting rule is L:ShareCopy:

\[
\inferr{
  \Sigma; \Gamma, x : A \sststile{q'}{q} \sharecpcst{x}{x_1}{x_2}{e} : B
}{
  A \;\curlyvee \;A_1, A_2,1\\
  \Sigma; \Gamma, x_1 : A_1, x_2 : A_2 \sststile{q'}{q} e : B
}(\text{L:ShareCopy})
\]

To share a variabe of type $A$, we need to split the potential between two new annotated types 
$A_1$ and $A_2$. In addition, we have to pay an ``overhead'' of 1 for every cons node 
in any structure in $\llbracket A \rrbracket$.

Lastly, since we are interested in the number of heap locations, 
there is an implicit side condition in all rules
which ensures all constants are assumed to be nonnegative.

\begin{mathpar}
\inferr{
  \Sigma; x : B \sststile{q}{q} x : B
}{
}(\text{L:Var})

\inferr{
  \Sigma; x : A \sststile{q'}{q} f(x) : B
}
{
  \Sigma(f) = A \xmapsto{q/q'} B
}

\inferr{
  \Sigma; \Gamma, x : \irl{bool} \sststile{q'}{q} \ifexcst{x}{e_t}{e_f} : B
}{
  \Sigma; \Gamma \sststile{q'}{q} e_t : B \\
  \Sigma; \Gamma \sststile{q'}{q} e_f : B
}(\text{L:Cond})

\inferr{
  \Sigma; x_1 : A_1, x_2 : A_2 \sststile{q}{q} \pairexcst{x_1}{x_2} : \prodtycst{A_1}{A_2}
}{
}(\text{L:Pair})

\inferr{
  \Sigma; \Gamma, x : (A_1,A_2) \sststile{q'}{q} \paircasecst{x}{x_1}{x_2}{e} : B
}{
  \Sigma; \Gamma, x_1 : A_1, x_2 : A_2 \sststile{q'}{q} e : B
}(\text{L:MatP})

\inferr{
  \Sigma; \emptyset \sststile{q}{q} \irl{nil} : L^p(A)
}{
}(\text{L:Nil})

\inferr{
  \Sigma; x_h : A, x_t : L^p(A) \sststile{q}{q+p+1} \consexcst{x_h}{x_t} : L^p(A)
}{
}(\text{L:Cons})

\inferr{
  \Sigma; \Gamma_1, \Gamma_2 \sststile{q'}{q} \irl{let}(e_1; x : \tau.e_2) : B
}{
  \Sigma; \Gamma_1 \sststile{p}{q} e_1 : A \\
  \Sigma; \Gamma_2, x : A \sststile{q'}{p} e_2 : B
}(\text{L:Let})

\end{mathpar}

For example, we can give the following typing derivation to append. 
Note that the program and signature is populated with global function declarations (in this case,
\texttt{append}).

\begin{verbatim}
let rec append (l1, l2) =
  match l1 with
    | [] -> l2
    | x::xs -> let r = append (xs, l2) in x::r
\end{verbatim}

\begin{align*}
	P &= [\texttt{append} \mapsto e_{\texttt{append}}]\\
	\Sigma &= [\texttt{append} \xrightarrow{q/q} \prodtycst{L^p(A)}{L^p(A)} \to \L^p(A)]
\end{align*}

Type derivation:
\begin{tiny}
\[
\infer[\text{L:MatL}]{
	\Sigma; l_1 : L^p(A), l_2 : L^p(A) \sststile{q}{q} 
		\listcaseexcst{l_1}{l_2}{x}{xs}{\irl{let}\; r = append(xs,l_2) \; \irl{in}\; x::r } : L^p(A)
}{
	\infer[\text{L:Var}]{
		\Sigma; l_2 : L^p(A) \sststile{q}{q} l_2 : L^p(A)
	}{
	}&
	\infer[\text{L:Let}]{
		\Sigma; l_2 : L^p(A), x : A, xs : L^p(A) \sststile{q}{q + p + 1} 
				\irl{let}\; r = append(xs,l_2) \; \irl{in}\; x::r : L^p(A)
	}{
		\infer[\text{L:App}]{
			\Sigma; L_2 : L^p(A), xs : L^p(A) \sststile{q+p+1}{q+p+1} append(xs,l_2) : L^p(A)
		}{
			append : \xrightarrow{q/q} \prodtycst{L^p(A)}{L^p(A)} \to \L^p(A) \in \Sigma
		}&
		\infer[\text{L:Cons}]{
			\Sigma; x : A, r : L^p(A) \sststile{q}{q+p+1} x::r : L^p(A)
		}{
		}
	}
}
\]
\end{tiny}

This can be read as \texttt{append} takes two lists, each with potential $p$ per element  
and a constant potential $q$, and returns a list with potential $p$ per element and constant 
potential $q$. This bound is tight and reflects the fact that append is constructing the 
concatenation ``in place'' by collecting the cells in $l_1$. Thus, append induces no overhead 
heap cells in addition to its arguments.

In previous versions of RAML, the typing judgment is parametrized by a \emph{cost metric} 
$m : \ms{res\_const \to \mathbb{Q}}$, which assigns a rational constant to 
the set of control-flow points (\ms{res\_const}). 
Since the cost for all program construct is zero save for the cons
data constructor, we elide the cost metric altogether. Although we defined the constructor to cost
1 heap location (as shown in L:Cons and L:MatL), it can be any constant as long as the introduction
and elimination rules agree on that constant. Thus we can extend the type system to accurately track
closure sizes and constructor which vary in size depending on the argument (more in 
\ref{sect:future-work}).

Futhermore, note that given a judgment $\Sigma;\Gamma \sststile{q'}{q} e : A$, there are 
infinitely many admissible judgments $\Sigma;\Gamma \sststile{q' + k}{q + k} e : A$ 
for all $k \in \mathbb{N}$. The analysis module will export the constraints generated 
by the type system into an LP-solver which minimizes all coefficients, so that the 
smallest admissible $q,q'$ will be used in the resulting symbolic bound.
%------------------------------------------------------------------------------

\section{Soundness of $\fogc$}

We seek to prove the following theorem: 

\begin{theorem}[Soundness]
\label{itm:soundness} let $H \vDash V : \Gamma$, $\Sigma; \Gamma \sststile{q'}{q} e : B$,
$V,H \; \vdash e \Downarrow v, H'$, and $H' \vDash v \mapsto a : A$.
Then $\forall C \in \mathbb{Q}^{+}$ and $\forall F,R \subseteq \ms{Loc}$,
if $|F| \ge \Phi_{V,H}(\Gamma) + q + C$,
then there exists a value $w$, and a freelist $F'$ s.t.
\begin{enumerate}
	\item $V,H,R,F \vdash^{\ms{free}} e \Downarrow w, Y', F'$
	\item $\veq{H'}{Y'}{v}{w}$
\end{enumerate}
\end{theorem}

Where $V,H \; \vdash e \Downarrow v, H'$ is a judgment from the operational semantics
derived from $\ms{free}$ and $\veq{H'}{Y'}{v}{w}$ is value equivalence. The theorem states that,
given a terminating expression,
and given a free-list that is sufficiently large (as predicated by the type derivation), 
a run with $\ms{free}$ will normalize to an equivalent value.

To facilitate the proof, we define an intermediate semantics $\ms{copy}$ 
which is semantically linear. $\ms{free}$. The plan has two stages: show $\ms{copy}$ over 
approximates $\ms{free}$, then show $\fogc$ is sound w.r.t. $\ms{copy}$:

\tikzstyle{process} = [rectangle, text centered, draw=black, fill=orange!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

\begin{center}
\begin{tikzpicture}[node distance=2.3cm]
\node (type) [process] {$\fogc$};
\node (copy) [process, right of=type, xshift=2cm] {\ms{copy}};
\node (free) [process, below of=copy] {\ms{free}};

\draw [arrow] (type) -- node[anchor=south] {Is sound w.r.t (\ref{itm:soundness})} (copy);
\draw [arrow] (copy) -- node[anchor=west] {Over-approximates (\ref{itm:frugal})} (free);
\draw [dashed,->] (type) -- node[anchor=south] {} (free);

\end{tikzpicture}
\end{center}

\subsubsection{The Garbage Collection Cost Semantics $\ms{copy}$}
\label{sect:copy}

In order to establish the soundness of the our intended semantics $\ms{free}$, we need an 
intermediary semantics which is \emph{semantically linear}: $\ms{copy}$.  
As mentioned in \ref{sect:fop}, this means that locations are linear, i.e. no location can be 
used twice in a program. Thus, variable sharing is achieved via \emph{copying}: 
the shared value is created by allocating a fresh set of locations from the freelist and copying the 
locations of the original value one by one. This is also sometimes referred to as deep copying.
Let $copy(H,L,v,H',v')$ be a 5-place relation on 
$\ms{Heap} \times \mathcal{P}(\ms{Loc}) \times \ms{Val} \times \ms{Heap} \times \ms{Val}$. 
Similar to reachability, we write this as 
$H',v = copy(H,L,v)$ to signify the intended mode for this predicate: $(+,+,+,-,-)$.
\begin{mathpar}
	\inferr{
		H,v = copy(H,L,v)	
	}{
		v \in \{n,\irl{T},\irl{F},\irl{Null}\}
	}

	\inferr{
		H'\{l' \mapsto v\},l' = copy(H,L,l) 
	}{
		l' \in L\\
		H',v = copy(H,L \setminus \{l'\},H(l)) 
	}

\inferr{
	H_2,\pairexcst{v_1'}{v_2'} = copy(H,L,\pairexcst{v_1}{v_2})
}{
	L_1 \sqcup L_2 \subseteq L\\
	|L_1| = |dom(reach_H(v_1)|\\
	|L_2| = |dom(reach_H(v_2)|\\
	H_1,v_1' = copy(H,L_1,v_1)\\
	H_2,v_2' = copy(H_1,L_2,v_2)\\
}
\end{mathpar}

Primitives require no cells to copy; a location value is copied recursively; 
a pair of values is copied sequentially, and the total number of cells required 
is the size of the reachable set of the value.  
Now, consider $\ms{free}$ with the share rule replaced with the following: 

\[
\inferrule{
	V(x) = v\\
  L \subseteq F\\
  |L| = |dom(reach_H(v'))|\\
  H',v'' = copy(H,L,v')\\
	V_2 = (V[x_1 \mapsto v',x_2 \mapsto v'']) \restriction_{FV(e)}\\
	F' =	F \setminus L\\
	g = \{l \in H \mid l \notin F' \cup R \cup locs_{V_2,H}(e)\}\\
 	V_2,H',R,F' \sqcup g \; \vdash e \Downarrow v,H'',F'
}{
  V,H,R,F \; \vdash \sharecpcst{x}{x_1}{x_2}{e} \Downarrow v,H'',F'
}(\text{E:Share})
\]

To share a variable, we first copy the shared value; the number of cells required is equal to the
size of the reachable set from the value. This copying sharing semantics is what justifies the 
analysis to use restitute the potential when matching on a cons node, 
since even if the node was shared, we had to pay for the cost by copying the node when sharing the 
original value. Next, we restrict the stack to the appropriate variables. 
Lastly, any locations not reachable from the current subexpression $e$ is collected. This is 
for the case when a variable is shared but not used later.  We list the rest of the (abridged) 
rules below:

Call this new semantics $\mathsf{copy}$ semantics for copy sharing
(all rules are renamed to E:\_). 
Intuitively, we expect that any terminating compuation
with $\mathsf{copy}$ has a corresponding run with $\mathsf{free}$ that can be instantiated with 
an equally-sized or smaller freelist. Although this seems quite straightforward to prove, a complete
characterization of the relationship of between the space allocations of two runs with each 
semantics is necessary. To motivate this, consider the following proof attempt: 

\begin{attempt}
	Let $\mathcal{C_2} = (V,H,R,F)$ be a configuration and $(\mathcal{C_2}, e)$ 
	be a linear computation. Given that 
	$\mathcal{C}_2 \vdash^{\mathsf{copy}} e \Downarrow v,H',F'$, and $H' \vDash v \mapsto a : A$, 
	for all configurations $\mathcal{C}_1 = (W,Y,R,M)$ such that $W,Y \sim V,H$ and $|M| = |F|$,
there is exists a triple
$(w,Y',M') \in \ms{Val} \times \ms{Heap} \times \ms{Loc}$ s.t.
	\begin{enumerate}
			\item $\mathcal{C}_1 \vdash^{\mathsf{free}} e \Downarrow w,Y',M'$
			\item $\veq{H'}{Y'}{v}{w}$
			\item $|M'| \ge |F'|$
	\end{enumerate}
\end{attempt}

We proceed with induction on the derivation of the judgment in $\ms{copy}$. 
Almost every case goes through, save for E:Let. 
First, we get $W_1,Y \sim V_1,H$ and We have the following from induction on the first premise:

\begin{enumerate}
	\item $W_1,Y,R',M \vdash^{\mathsf{free}} e \Downarrow w_1,Y_1,M_1$
	\item $\veq{H_1}{Y_1}{v_1}{w_1}$
	\item $|M_1| \ge |F_1|$
\end{enumerate}

To instantiate the induction hypothesis on the second premise, we need to show that, among 
other things, $|M_1 \cup j| \ge |F_1 \cup g|$, where $j$ is the set of collected locations in 
the $\ms{free}$ judgment. We cannot show this precisely because $g$ might contain more cells 
then $j$ due to the linearity of $\ms{copy}$, thus preventing a piecewise comparison. 
But of course $|j|$ is always less than $|g|$, since $\ms{free}$ doesn't copy to share 
values! This shows that there is a mismatch between the induction hypothesis and the relationship
between the sizes of the respective free-lists and the garbage sets. We present the following 
criteria which characterizes the required equivalence between two configuration,
called \emph{copy extension}.

\begin{definition}
A configuration $\mathcal{C}_2 = (V_2,H_2,R_2,F_2)$ is a \emph{copy extension} of another configuration
$\mathcal{C}_1 = (V_1,H_1,R_1,F_1)$ iff
\begin{enumerate}
\item $V_1,H_1 \sim V_2,H_2$
\item There is a proper partition $\gamma : dom(H_1) \setminus F_1 \to \mathcal{P}(dom(H_2) \setminus F_2)$ 
such that for all $l \in dom(\gamma)$, $|\gamma(l)| = reach_{H_1}(V_1)(l) + R_1(l)$
\item for all $l \in dom(\gamma)$ and $x \in dom(V_1)$, 
	$|reach_{H_2}(x) \cap \gamma(l)| = reach_{H_1}(V_1(x))(l)$.
\item $R_1 \subseteq dom(H_1) \setminus F_1$ and 
	$R_2 = \bigcup \gamma(R_1) \setminus reach_{H_2}(V_2)$
\item $|F_1| = |F_2| + |\oh{\gamma}|$, where 
	$\oh{\gamma} = \bigcup_{P \in ec(\gamma)} P \setminus (rep(P))$
\end{enumerate}
Write this as $\mathcal{C}_1 \preceq \mathcal{C}_2$.
\end{definition} 

The intention is that $\mathcal{C}_2$ is a configuration running with $\ms{copy}$, and 
$\mathcal{C}_1$ a configuration running with $\ms{free}$. 

The first condition is the straightforward context equivalence.

The second condition requires the existence of 
a mapping $\gamma$ that tells us given a location in $H_1$, which locations in $H_2$ 
are shared instances. For example, consider the expression: 

\begin{verbatim}
share x as x1, x2 in e
\end{verbatim}

Where the stack is $[\texttt{x} \mapsto 1]$, and the heap equals 
$[1 \mapsto \pairexcst{0}{\irl{Null}}]$, i.e. \texttt{x} is the list \texttt{[0]}.
Running with $\ms{free}$, the stack becomes 
$[\texttt{x1} \mapsto 1,\texttt{x2} \mapsto 1]$, and the heap does not change. With 
$\ms{copy}$, we allocate a new location in the heap:
$[1 \mapsto \pairexcst{0}{\irl{Null}},2 \mapsto \pairexcst{0}{\irl{Null}}]$, and the 
stack changes accordingly: $[\texttt{x1} \mapsto 1,\texttt{x2} \mapsto 2]$.
Now $\gamma$ would map 1 to $\{1,2\}$, since both are shared instances of the former.

Thus, each $\gamma(l)$ is mapped to a disjoint subset in $H_2$, and each location in $H_2$ 
would have a representative in $H_1$. Furthermore, we noticed it is crucial to include the 
fact that the size of $\gamma(l)$ must be the sum number of references from the stack and 
the continuation set. 

While $\gamma$ gives us a relation between the two respective heaps, we still need to know 
how variables on the stack contribute to the number of ways to reach a location in $H_1$. 
The third condition gives this more fined grained restriction: for each  $\gamma(l)$, 
the size of the part reachable from $x$ in $H_2$ is exactly the number of ways $x$ can 
reach $l$ in $H_1$. This condition is needed to ensure that when sequencing evaluations, 
the garbage collected expression has the expected reachable set.

The next condition simply states that the continuation sets respect $\gamma$. Lastly, 
we have have that $F_1$ is greater $F_2$, with the overhead being exactly the sum 
$\sum_{l \in \gamma} |\gamma(l)| - 1$. 

\begin{definition}
For an evaluation $\mathcal{C} = (V,H,R,F) \vdash e \Downarrow v,H',F'$, denote its garbage as
$collect(R,v,H',F') = \{l \in H' \mid l \notin F' \cup R \cup reach_H'(v)\}$. 
\end{definition}

\begin{definition} A configuration $(V,H,R,F)$ is well-formed if 
	$dom(H) \subseteq reach_H(V) \cup R \cup F$.
\end{definition}
Now the key lemma:

\begin{lemma}\label{itm:frugal}
	Let $(\mathcal{C}_2,e)$ be a linear computation. Given that 
	$\mathcal{C}_2 \vdash^{\mathsf{copy}} e \Downarrow v,H',F'$, and $H' \vDash v \mapsto a : A$, 
	for all well-formed configurations $\mathcal{C}_1$ such that $\mathcal{C}_1 \preceq \mathcal{C}_2$,
there is exists a triple
$(w,Y',M') \in \ms{Val} \times \ms{Heap} \times \ms{Loc}$ and 
	$\gamma' : dom(Y') \setminus M' \to \mathcal{P}(dom(H') \setminus F')$ s.t.
	\begin{enumerate}
			\item $\mathcal{C}_1 \vdash^{\mathsf{free}} e \Downarrow w,Y',M'$
			\item $\veq{H'}{Y'}{v}{w}$
			\item $\gamma'$ is a proper partition, such that for all $l \in dom(\gamma')$, 
				$|\gamma'(l)| = reach_{Y_1}(w_1)(l) + S(l) + |\gamma'(l) \cap collect(R,v,H',F')|$
			\item $|reach_{H_1}(v) \cap \gamma'(l)| = reach_{Y_1}(w)(l)$
			\item $R_2 = \bigcup \gamma'(R_1) \setminus reach_{H_2}(V_2)$ 
			\item $|M'| = |F'| + |\oh{\gamma'}|$
	\end{enumerate}
\end{lemma}

The third condition deserves some explanation. The size of $\gamma(l)$ should equal the total number 
of ways $l$ could be reached from the root set in the $\ms{free}$ evaluation. This includes 
$reach_{Y_1}(w_1)(l)$ and $S(l)$, but also any paths to $l$ had been collected during the run 
(this accounts for unused variables that referenced $l$).

For a configuration $\mathcal{C} = (V,H,R,F)$, denote the current garbage w.r.t a set of root variables 
$X \subseteq dom(V)$ 
as $clean(\mathcal{C},X) = \{l \in H \mid l \notin F \cup R \cup reach_H(X)\}$. Some auxiliary lemmas: 

\begin{lemma}\label{itm:aux}
Let $V_2,H_2,R_2,F_2 \vdash^{\mathsf{copy}} e \Downarrow v,H',F'$, and 
$V_1,H_1,R_1,F_1 \preceq V_2,H_2,R_2,F_2$ because $(-,\gamma,\eta,-,-)$. Then the following hold:
\begin{enumerate}
\item for all $l \in dom(H_1) \setminus F_1$, 
$X \subseteq dom(V)$, $\gamma(l) \subseteq clean(\mathcal{C}_2,X)$ implies that 
$l \in clean(\mathcal{C}_1,X)$.
\end{enumerate}
\end{lemma}

Thus, we have shown that we can execute a computation using the 
$\ms{free}$ if the computation suceeded in a run with $\ms{copy}$, and that indeed 
$\ms{copy}$ is an over approximation of $\ms{free}$.

\subsubsection{Soundness of $\ms{copy}$}

To formally state the soundness theorem,
we need to introduce an auxiliary operational semantics (call this \ms{oper}) 
that does not use freelists or accounts for garbage collection. We use
it to characterize expressions that normalize to values when initialized with a sufficient 
freelist.  This technique was also employed in \cite{Hofmann:2003:SPH:604131.604148} 
to establish the soundness of their type system. The operational semantics consists of 
judgments of the following form, essentially a simplified version of the full cost semantics:

\[
\fbox{$V,H \vdash e \Downarrow v, H'$}
\]

This can be read as: under stack $V$, heap $H$ the expression $e$ evaluates to $v$, 
and engenders a new heap $H'$. The rules are entirely standard and are omitted here.
In $\ms{oper}$, the ``freelist'' is the whole ambient set of locations $\ms{Loc}$, 
thus we never run out of locations during evaluation. This introduces a problem when comparing 
evaluation results between a run with $\ms{copy}$ and $\ms{oper}$, as the return values 
might not be syntatically equal. Consider the following expression:

\begin{verbatim}
let _ = [4] in [5]
\end{verbatim}

Assuming locations are natural numbers, and we run $\ms{copy}$ with the freelist $\{1\}$. 
First, 1 is allocated and mapped to $\texttt{[4]}$. 
Then, since the first subexpression $\texttt{[4]}$ is not used afterwards, we collect 1, and reuse it
and map again to $\texttt{[5]}$. Thus the return value is 1.  In a run with $\ms{oper}$, 
we also first map 1 to $\texttt{[4]}$, but then 
allocate a new location, say 2, and map it to $\texttt{[5]}$, and the return value is 2. Due to the 
difference in allocation strategies and the fact that both are nondeterministic, we need a more 
robust notion of equality for values. Luckily, the structures from the denotational semantics 
does the job. In both runs, the return value maps to the semantic value $\texttt{[5]}$.
Thus semantical equality serves as the basis for context equivalence. 
Here we define it for contexts, which consisting of only the stack and heap. 
Later, we extend it the the full configuration. First, define value equivalence as
a shorthand for semantic equality: 

\begin{definition}[Value Equivalence]
Two values $v_1,v_2$ are equivalent (with the presupposition that they are well-formed w.r.t heaps $H_1,H_2$),
iff $H_1 \vDash v_1 \mapsto a : A$ and $H_2 \vDash v_2 \mapsto a : A$. 
Write value equivalence as $\veq{H_1}{H_2}{v_1}{v_2}$.
\end{definition}

\begin{definition}[Context Equivalence]
Two simple contexts $(V_1,H_1), (V_2,H_2)$ are equivalent
(with the presupposition that both are well-formed contexts) iff $dom(V_1) = dom(V_2)$ and 
for all $x \in dom(V_1)$, $\veq{H_1}{H_2}{V_1(x)}{V_2(x)}$. Write context equivalence as 
$\ctxeq{V_2,H_2}{V_2,H_2}$
\end{definition}

Stated simply, two contexts are equivalent when they have the same domain and equal variables bind 
equal semantic values. 

\subsubsection{Denotational Semantics}

The last ingredient we need before stating the soundness theorem is defining linear potential.
Since potential is associated with the \emph{structure} of a value and not the particular heap 
locations, we need a mapping from
heap values to semantic values (or structures) of a type. 
First give a denotational semantics for (define the structures of) the first-order types: 

\begin{align*}
	\denote{\unittyabt} &= \{\val{\irl{Null}}\}\\
	\denote{\booltyabt} &= \{\val{\irl{T}}, \val{\irl{F}}\}\\
	\denote{\irl{nat}} &= \mathbb{N}\\
\pairexcst{a_1}{a_2} &\in \denote{\prodtycst{A_1}{A_2}} 
	\text{ if } a_1 \in \denote{A_1} \text{ and } a_2 \in \denote{A_2}\\
\nilexcst &\in \denote{L(A)}\\
\consexcst{a}{l} &\in \denote{L(A)} \text{ if } a \in \denote{A} \text{ and } l \in \denote{L(A)}\\
\end{align*}

Where semantic set for each type is the least set such that the above holds. Write $[a_1,...,a_n]$ for $\consexcst(a_1,...,\consexcst(a_n,\nilexcst))$.
We also refer to the elements of a semantic set as structures. \\

Now we give the judgements relating heap values to semantic values, in the form \fbox{$H \vDash v \mapsto a : A$}, which can be read as: under heap $H$, heap value $v$ defines the semantic value $a \in \denote{A}$.  

\begin{mathpar}
\inferr{
  H \vDash \val{n} \mapsto n : \irl{nat}
}{
  n \in \mathbb{Z}
}(\text{V:ConstI})

\inferr{
	H \vDash \val{\irl{Null}} \mapsto \val{\irl{Null}} : \unittyabt
}{
}(\text{V:ConstI})

\inferr{
  H \vDash \val{\irl{Null}} \mapsto \val{\irl{Null}} : L(A)
}{
  A \in \ms{BType}
}(\text{V:Nil})

\inferr{
  H \vDash \val{\irl{T}} \mapsto  \val{\irl{T}} : \booltyabt
}{
}(\text{V:True})

\inferr{
  H \vDash \val{\irl{F}} \mapsto \val{\irl{F}}  : \booltyabt
}{
}(\text{V:False})

	\inferr{
		H \vDash \pairexcst{v_1}{v_2} \mapsto \pairexcst{a_1}{a_2} : \prodtycst{A_1}{A_2}
	}{
		H \vDash v_1 \mapsto a_1 : A_1 \\
		H \vDash v_2 \mapsto a_2 : A_2
	}(\text{V:Pair})
	
\inferr{
  H \vDash l \mapsto [a_1,\ldots,a_n] : L(A)
}{
  l \in \ms{Loc}\\
  H(l) = \pairexcst{v_h}{v_t}\\
  H \vDash v_h \mapsto a_1 : A\\
  H \vDash v_t \mapsto [a_2,\ldots,a_n] : L(A)
}(\text{V:Cons})
\end{mathpar}

Given a stack $V$, we write $H \vDash V : \Gamma$ if $dom(V) = dom(\Gamma)$ and
for every $x : A \in \Gamma$, $H \vDash V(x) \mapsto a : A$ for some $a \in \llbracket A \rrbracket$.

\subsubsection{Linear Potential}
We introduce linear potential for structures corresponding to the base types. The definition of
linear potential is standard material, and can be found in \ref{Hoffmann11}, which is also
a good source for an introduction to AARA.
With linear potential, each component of a structure is associated with a constant amount of 
potential.  Given a structure $a$ in a heap $H$, where  $H \vDash v \mapsto a : A$, we define 
its potential $\Phi_H(a : A)$ by recursion on $A$: 

\begin{align*}
&\Phi_H(a : A) = 0 &A \in \{\unittycst, \booltyabt, \irl{nat}\}\\
&\Phi_H(\pairexcst{a_1}{a_2} : \prodtycst{A_1}{A_2}) = \Phi_H(a_1 : A_1) + \Phi_H(a_2 : A_2)\\
&\Phi_H([a_1,...a_n] : L^p(A)) = p\cdot n + \sum_{1 \le i \le n} \Phi_H(a_i : A)  
\end{align*}

Now define $A \curlyvee A_1,A_2,n$ as the sharing relation for resource-annotated types:
\begin{align*}
	&L^p(A) \curlyvee^n L^q(A_1),L^r(A_2) & \text{if } p = q + r + n \;\text{and}\; 
			A \curlyvee^n A_1,A_2\\
	&\prodtycst{A}{B} \curlyvee^n \prodtycst{A_1}{B_1}, \prodtycst{A_2}{B_2}
		&\text{ if } A \curlyvee^n A_1,A_2 \text{ and } B \curlyvee^n B_1,B_2\\
	&A \curlyvee^n  A,A& \text{ if } A \in \{\unittycst, \booltycst, \irl{nat}\}\\
\end{align*}
The sharing relation captures the amount of potential needed to copy a type $A$ where each 
cons node in any structure in $\llbracket A \rrbracket$ has a copying overhead $n$.

We are ready to state the soundness theorem now:

\begin{theorem}[Soundness]
\label{itm:soundness} let $H \vDash V : \Gamma$, $\Sigma; \Gamma \sststile{q'}{q} e : B$,
$V,H \; \vdash e \Downarrow v, H'$, and $H' \vDash v \mapsto a : A$.
Then $\forall C \in \mathbb{Q}^{+}$ and $\forall F,R \subseteq \ms{Loc}$,
if the following holds:
\begin{enumerate} 
\item $\wfc{V}{H}{R}{F}{e}$
\item $|F| \ge \Phi_{V,H}(\Gamma) + q + C$ 
\end{enumerate}
then there exists a context $(W,Y)$, a value $w$, and a freelist $F'$ s.t.
\begin{enumerate}
	\item $\ctxeq{W,Y}{V,H}$
  \item $W,Y,R,F \vdash e \Downarrow w, Y', F'$
	\item $\veq{H'}{Y'}{v}{w}$
  \item $|F'| \ge \Phi_{H'}(v:B) + q' + C$
\end{enumerate}
\end{theorem}

In other words, given a terminating expression (verified by succeeding with the run using \ms{oper})
and given a free-list that is sufficiently large (as predicated by the type derivation), 
a run with $\ms{copy}$ will normalize to an equivalent value, and the resulting free-list 
will be sufficiently large (as predicated by the type derivation).

\subsubsection{Linearity of Copy Semantics}

In the soundness proof of $\fogc$, we used an important lemma: that $\ms{copy}$ is 
semantically linear, i.e. locations are used linearly. 
To see why, consider the second premise in the rule L:MatL. In addition to the 
$p$ units of potential justified by the definition of linear potential, we get 1 unit 
from deallocating the cons cell itself. This is only sound if in the corresponding rule in 
$\ms{copy}$ a location was actually collected. Consider the evaluation in question:

\[
\infern{
  V(x) =  l\\
  H(l) = \pairexcst{v_h}{v_t} \\
	V'' = (V[x_h \mapsto v_h, x_t \mapsto v_t])\restriction_{FV(e_2)}\\
  g = \{l \in H \mid l \notin F \cup R \cup locs_{V'',H}(e_2)\}\\
  V'',H,R,F \cup g \; \vdash e_2 \Downarrow v, H',F' \\
}{
  V,H,R,F \; \vdash \listcaseexcst{x}{e_1}{x_h}{x_t}{e_2} \Downarrow v,H',F'
}
\]

If all the variables in $V$ was mapped to values with disjoint reachable sets, 
then we see that $l$ is only in the reachable set of $x$ (assuming that well-typed expressions
don't have duplicate occurences of variables, i.e. $x \notin FV(e_1) \cup FV(e_2)$. 
Then it follows that $l \in g$ given that locations in $V$, $R$, and $F$ are also all disjoint, 
and this is what we needed to justify the rule L:MatL.
Thus we have to show that $\ms{copy}$ preserves the linearity invariant: given a \emph{linear} 
computation, the evaluation result is also linear. 

First, we characterize semantically linear contexts: 

\begin{definition}(Linear context)
Given a context $(V,H)$, let
$x,y \in dom(V)$, $x \ne y$, and $r_x = reach_H(V(x))$, $r_y = reach_H(V(y))$.
	It is \emph{linear} given that:
\begin{enumerate}
\item $\ms{set}(r_x), \ms{set}(r_y)$
\item $r_x \cap r_y = \emptyset$
\end{enumerate}
Denote this by $\na{V,H}$.
\end{definition}

Whenever $\na{V,H}$ holds, visually, one can think of the 
stack as a collection of disjoint, directed trees with locations as nodes; 
consequently, there is at 
most one path from a variable on the stack $V$ to any location in $H$. Now we can 
formalize our intuition for linear computations: 

\begin{definition}[Linear computation]
Given a configuration $\mathcal{C} = (V,H,R,F)$ and an expression $e$, 
we say the 5-tuple $(\mathcal{C},e)$ is a \emph{computation}; it is a \emph{linear computation} 
given the following:
\begin{enumerate}
\item $dom(V) = FV(e)$
\item $\na{V,H}$
\item $\dist{\{R,F,locs_{V,H}(e)\}}$
\end{enumerate} 
And we write $\wfc{V}{H}{R}{F}{e}$ to denote this fact.
\end{definition}


% main lemma
The following lemma one of main results of this paper: given a semantically linear computation 
(one with no sharing between the underlying locations), the resulting value is linear 
(expressed by item 1. and 2. below):
\begin{lemma}[Linearity of $\ms{copy}$]\label{itm:na}
For all stacks $V$ and heaps $H$, let  $V,H,R,F \; \vdash e \Downarrow v, H', F'$ 
and $\Sigma; \Gamma \vdash e : B$. Then given that $\wfc{V}{H}{R}{F}{e}$, we have the follwoing: 
\begin{enumerate}
\item $\ms{set}(reach_{H'}(v))$
\item $\dist{\{R,F',reach_{H'}(v)\}}$, and
\item $\stable{R,H,H'}$
\end{enumerate}
\end{lemma}

Where $\ms{stable}$ is a predicate on $\mathcal{P}(\ms{Loc}) \times \ms{Heap} \times \ms{Heap}$, defined
below. The premises of this lemma is a subset of the premises of the soundness theorem. 
Thus, we could have
merged the proof of this lemma directly into the soundness proof. However, we think makes the 
presentation clearer; furthermore, the linearity of $\ms{copy}$ is an interesting in itself, 
regardless of the accompanying type system. 

We define some auxiliary lemmas used to prove the main lemma: 
\begin{definition}[Stability]
Given heaps $H,H'$, a set of locations is \emph{stable} if $\forall l \in R$. $H(l) = H'(l)$. Denote this by
$\stable{R,H,H'}$.
\end{definition}

Define $\dagger :  L^p(A) \mapsto L(A)$ as the map that erases resource annotations. 
This gives a simplified jugdment \fbox{$\Sigma^{\dagger}; \Gamma^{\dagger} \vdash e : B^{\dagger}$}
used in proofs where the resource annotations are not necessary.

\begin{lemma}
\label{a} If $\Sigma; \Gamma \sststile{q'}{q} e : B$, then $\Sigma^{\dagger}; \Gamma^{\dagger} \vdash e : B^{\dagger}$.
\end{lemma}

\begin{proof}
Induction on the typing judgement.
\end{proof}

Define $FV^{\star} : \ms{Exp} \to \wp(\ms{Var})$, the multiset of free variables of expressions,
as the usual $FV$ inductively over the structure of $e$. This version of $FV$ reflects 
the multiplicity of variable occurences.

\begin{lemma}\label{itm:linear}
\label{a} If $\Sigma; \Gamma \sststile{q'}{q} e : B$, then $\set{FV^{\star}(e)}$ and $dom(\Gamma) = FV(e)$.
\end{lemma}

\begin{proof}
Induction on the typing judgement.
\end{proof}

\begin{lemma}\label{itm:stable}
Let $H \vDash v \mapsto a : A$. For all sets of locations $R$, if $reach_H(v) \subseteq R$ and $\stable{R,H,H'}$, then $H' \vDash v \mapsto a : A$ and $reach_H(v) = reach_{H'}(v)$.
\end{lemma}

\begin{proof}
Induction on the structure of $H \vDash v \mapsto a : A$.
\end{proof}

\begin{corollary}
Let $H \vDash V : \Gamma$. For all sets of locations $R$, if $\bigcup_{x \in V} reach_H(V(x)) \subseteq R$ and $\stable{R,H,H'}$, then $H' \vDash V : \Gamma$.
\end{corollary}

\begin{proof}
Follows from Lemma $\ref{itm:stable}$.
\end{proof}

% copy

\begin{lemma}[stability of copying]
	Let $H',v' = copy(H,L,v)$. For all $l \in H$, if $l \notin L$, then $H(l) = H'(l)$. 
	Further, $reach_{H'}(v') \subseteq L$.
\end{lemma}

\begin{lemma}[copy is copy]
	Let $H',v' = copy(H,L,v)$. If $H \vDash v \mapsto a : A$, then $H' \vDash v' \mapsto a : A$.
\end{lemma}


%------------------------------------------------------------------------------


\section{Implementation}
\label{sect:implementation}

\jan{Implementation of analysis}
The garbage collection cost semantics is implemented as an alternative evaluation module inside
Resource Aware ML (RaML). As mentioned before, RaML leverages the syntax of OCaml programs. 
First, we take the OCaml type checked abstract syntax tree and perform a series of transformations. 
The evaluation modules operates on the resulting RaML syntax tree. 
In the gc evaluation module, \texttt{evaluate} has the following signature:

\begin{verbatim}
 evaluate : 
    ('a, unit) Expressions.expression -> int ->
  (('a value * 'a heap * Int.Set.t) option) 
\end{verbatim}

Where the second argument \texttt{int} specifies the size of the initial free-list.
The result is a triple of the return value, heap, and free-list, or \texttt{None} 
in case the freelist was not sufficient for the evaluation.
Whereas the normal evaluation boxes every value (everything evaluates to a location), 
the gc module follows the cost semantics and only boxes data constructors. The rationale is
that the size for other values can be computed statically and stack allocated. One difference
between the cost semantics and its implementation is that while in the language presented here
list is the only data type, our implementation supports user defined data types. The extension
is straightforward except the treatment of the nil constructor, or generally ``empty'' constructors
that has arity zero. For simplicity of presentation, we evaluate all nil constructors to
the same null value in the cost semantics. This is natural for lists because all nil constructors 
are the same, and every list has at most one nil node. However, for custom data types that have 
more than one kind of empty constructor, it is not possible to map every constructor to the same 
null value. Thus, the implementation treats all constructors uniformly, so each nil constructor
also cost one heap location. 

As mentioned before, all functions used in a program are declared in a global mutually 
recursive block, and we do not account for the constant space overhead for this block in 
the cost semantics.  In order to implement this global function block, we allow closure creation
during program evaluation. However, we allocate all closures from a separate freelist into 
a separate heap.  This ensures that data constructors are allocated from the correct freelist 
and no space overhead is created by allocating closures for function declarations.

\subsubsection{Evaluation}
\label{sect:evaluation}

We evaluated our new analysis on a number of functions, as seen in the tables below. 
The first table shows the type signature for each function. The second table presents the test 
data that
showcase the difference between the heap metric, the old analysis which only counts 
heap allocations, and the gc metric, which includes deallocations and copying cost for sharing.
For each metric, we show the heap space bound computed by RaML, its asymptotic behavior, 
the number of constraints generated, and the time elapsed during analysis. The last column gives
the expression for the exact heap high watermark derived by hand and verified by running the 
cost semantics.

Except for bfs and dfs, all functions in the table take a \emph{principal} argument of type list. 
The variables in the table refer to this argument (for example, the type of the 
principal argument of 
quicksort is 'a list). In general, M refers to the number of cons constructors of the principal 
argument (or the number of \emph{outer} cons nodes in case of nested lists); L refers to the
maximum number of cons nodes of the inner lists; N refers to the number of nil constructors.

For the sorting functions aside from mergesort, the new analysis using the gc metric
derived asymptotically better bounds when compared to the heap metric. Furthermore,
all bounds are \emph{exact} with respect to the cost semantics. In regards to mergesort, 
the analysis was not able to derive a tight bound due to the limitations of AARA in dealing 
with logarithmic bounds. A particularly nice result is that we can deduce for quicksort, 
the space usage is exactly 0, which justifies its use as a zero space-overhead sorting algorithm.

Next, we have have the graph search algorithms operating on a binary tree. Again, the gc metric
was able to derive exact space overheads, while the heap metric derived linear bounds for both.

For transpose, the gc metric derived an asymptotically better bound, but was not able to derive 
the exact overhead. We implement matrices as lists-of-lists in row-major order. 
The transpose function is implemented tail-recursively, with the accumulator starting as the 
empty list. When ``flipping'' the first row $r$ of the input and appending this to the accumulator, 
we need to create $|r|$ many new nil and cons constructors to store the row as a column. While this
overhead only occurs once, RaML is unable to infer this from the source code, and thus the cost
is repeated over the entire input matrix, resulting in the linear bound (w.r.t the size of the 
matrix). 

The last two functions demonstrate how the gc metric performs when there is variable sharing.
map\_it maps the input function across each list in the principal argument twice, returning a
tuple of nested lists. The gc metric dictates that every outer data constructor in the 
principal argument needs to be copied, and thus gives the linear bound M + N. In this case,
the bound is exact. The pairs functions takes a list and outputs a all pairs of the input list 
which are ordered ascending in input position. For example, 
pairs [1;2;3;4] = [(1,2);(1,3);(1,4);(2,3);(2.4);(3,4)]. For pairs, the gc metric derived a bound
that is asymptotically the same as the heap metric, but with better constants. An exact bound 
could not be derived because the deallocation potential from the pattern match in the 
definition of pairs is wasted because the matched body could already be typed with zero cost. 
However, this deallocation is used as usual in the cost semantics. Thus the slack in the bound 
totals to the size of the input. This example shows a weakness in the new gc metric and analysis; 
we discuss possible solutions and different approaches in the next sections. 

\begin{table}[]
	\begin{tabular}{ll}
		\hline
		\textit{\textbf{function}} & type \\ \hline
		quicksort & {[}'a -\textgreater 'a -\textgreater bool; 'a list{]} -\textgreater 'a list \\
		mergesort & {[}{[}'a; 'a{]} -\textgreater bool; 'a list{]} -\textgreater 'a list \\
		ocamlsort & {[}{[}'a; 'a{]} -\textgreater bool; 'a list{]} -\textgreater 'a list \\
		selection sort & int list -\textgreater int list \\
		eratosthenes & int list -\textgreater int list \\
		dfs & {[}btree; int{]} -\textgreater btree option \\
		bfs & {[}btree; int{]} -\textgreater btree option \\
		transpose & 'a list list -\textgreater 'a list list \\
		map\_it & {[}'a -\textgreater 'b; 'a list list{]} -\textgreater 'b list list * 'b list list \\
		pairs & 'a list -\textgreater ('a * 'a) list \\ \hline
	\end{tabular}
\end{table}

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table}[]
	\begin{adjustbox}{width=1.2\textwidth,center=\textwidth}
  \begin{tabular}{@{}llllllllll@{}}
		\toprule
		 & \multicolumn{4}{l}{heap metric} & \multicolumn{4}{l}{gc metric} &  \\ \midrule
		 \textit{\textbf{function}} & computed bound & asymptotic & constraints & time & computed bound & asymptotic & constraints & time & optimal \\
		 quicksort & 1.00 + 3.50*M + 1.50*M\textasciicircum{}2 & O(M\textasciicircum{}2) & 8515 & 0.52 & 0 & O(1) & 8519 & 0.48 & 0 \\
		 mergesort & 1.00 - 4.67*M + 6.33*M\textasciicircum{}2 & O(M\textasciicircum{}2) & 9572 & 0.64 & -0.50*M + 0.50*M\textasciicircum{}2 & O(M\textasciicircum{}2) & 9578 & 0.58 & \textbackslash{}floor(\textbackslash{}log(M)) \\
		 ocamlsort & 7.50 + 5.50*M + 1.00*M\textasciicircum{}2 & O(M\textasciicircum{}2) & 8565 & 0.51 & 1.00 + 1.00*M & O(M) & 8573 & 0.50 & M+1 \\
		 selection sort & 2.00 + 3.00*M + 1.00*M\textasciicircum{}2 & O(M\textasciicircum{}2) & 639 & 0.06 & 0 & O(1) & 642 & 0.05 & 0 \\
		 eratosthenes & 1.00 + 1.50*M + 0.50*M\textasciicircum{}2 & O(M\textasciicircum{}2) & 515 & 0.06 & 0 & O(1) & 517 & 0.04 & 0 \\
		 dfs & 3.00 + 2.00*M & O(M) & 5481 & 0.90 & 2 & O(1) & 5483 & 0.36 & 2 \\
		 bfs & 5.00 + 10.00*M & O(M) & 24737 & 4.15 & 4 & O(1) & 24742 & 1.62 & 4 \\
		 transpose & 1.00 + 3.50*L*M + 0.50*L*M\textasciicircum{}2 & O(L*M\textasciicircum{}2) & 10680 & 0.50 & 1.00 + 2.00*L*M & O(L*M) & 10684 & 0.50 & max(0,2*L-1) \\
		 map\_it & 2.00 + 2.00*L*M + 4.00*M & O(L*M) & 30699 & 1.58 & 1.00*M + 1.00*N & O(M+N) & 30703 & 1.57 & M + 1 \\
		 pairs & 1.00 + 1.00*M\textasciicircum{}2 & O(M\textasciicircum{}2) & 10214 & 0.60 & -0.50*M + 1.00*M*N + 0.50*M\textasciicircum{}2 & O(M\textasciicircum{}2+M*N) & 10217 & 0.64 & 0.5*M\textasciicircum{}2-1.5*M+2 \\
		  &  &  &  &  &  &  &  &  &  \\ \bottomrule
	\end{tabular}
	\end{adjustbox}
\end{table}

\section{Future Work}
\label{sect:future-work}

\jan{Related work}

\label{sect:bib}
\bibliographystyle{plain}
%\bibliographystyle{alpha}
%\bibliographystyle{unsrt}
%\bibliographystyle{abbrv}
\bibliography{easychair}

%------------------------------------------------------------------------------
\appendix


%------------------------------------------------------------------------------
% Index
%\printindex

%------------------------------------------------------------------------------
\end{document}


%%% Local Variables:
%%% mode: latex
%%% mode: flyspell
%%% TeX-master: t
%%% End:
